Pipelines
==========

Generating a config file and running the pipeline
*************************************************

This pipeline can be configured and run on custom data with a few steps.
The recommended method is to first run ``nextflow config ...`` to generate a complete config file (with the default parameters) in your working directory.
The tool-specific parameters, as well as Docker/Singularity profiles, are included when specifying the appropriate profiles to ``nextflow config``.

1. First, update to the latest pipeline version (this will update the nextflow cache of the repository, typically located in ``~/.nextflow/assets/vib-singlecell-nf/``)::

    nextflow pull vib-singlecell-nf/vsn-pipelines


2. Next, a config file needs to be generated.
   This step will merge parameters from multiple profiles together to create a master config which specifies **all** parameters used by the pipeline.
   In this example, these are ``tenx`` for the input data, ``singularity`` to use the Singularity system (replace with ``docker`` if necessary), and ``single_sample`` to load the defaults for the single sample pipeline.
   In your working directory, run ``nextflow config ...`` with the appropriate profiles::

    nextflow config vib-singlecell-nf/vsn-pipelines \
        -profile tenx,singularity,single_sample > single_sample.config



3. Now, edits can be made to ``single_sample.config``.
   Generally, the default values are acceptable to use for a first pass, but certain variables (input directory, etc.) need to be changed.

   In particular, the following parameters are frequently modified in practice:

    * ``params.global.project_name``: a project name which will be included in some of the output file names.
    * ``params.data.tenx.cellranger_mex``, which should point to the ``outs/`` folder generated by Cell Ranger (if using 10x data). See ``Information on using 10x Genomics datasets`` for additional info.
    * Filtering parameters (``params.sc.scanpy.filter``): filtering parameters, which will be applied to all samples, can be set here: min/max genes, mitochondrial read fraction, and min cells. See ``Multi-sample parameters`` for additional info on how to specify sample-specific parameters.
    * Louvain cluster resolution: ``params.sc.scanpy.clustering.resolution``.
    * For cell- and sample-level annotations, see ``here`` for additional info.


4. Run the workflow using the new config file (using ``-C`` is recommended to use **only** this file), specifying the proper workflow as the entry point::

    nextflow -C single_sample.config \
        run vib-singlecell-nf/vsn-pipelines \
        -entry single_sample


Two-pass strategy
---------------------

Typically, cell- and gene-level filtering is one of the first steps performed in the analysis pipelines.
This usually results in the pipeline being run in two passes.
In the **first pass**, the default filters are applied (which are probably not valid for new datasets), and a separate QC report is generated for each sample.
These QC reports can be inspected and the filters can be adjusted in the config file
either for all samples (by editing the ``params.sc.scanpy.filter`` settings directly, or for individual samples by using the strategy described in multi-sample parameters.
Then, the **second pass** restarts the pipeline with the correct filtering parameters applied (use ``nextflow run ... -resume`` to skip already completed steps).

Other notes
----------------
In order to run a specific pipeline (e.g. ``single_sample``),
the pipeline name must be specified as a **profile** when running ``nextflow config ...`` (so that the default parameters are included),
and as the **entry** workflow when running the pipeline with ``nextflow run``.

One exception to this is that the ``-entry`` pipeline can be one that is a subset of the one present in the config file.
For example, in a pipeline with long running step that occurs after filtering (e.g. ``single_sample_scenic``),
it can be useful to generate the full config file (``nextflow config vib-singlecell-nf/vsn-pipelines -profile single_sample_scenic``),
then run a first pass for filtering using ``nextflow run vib-singlecell-nf/vsn-pipelines -entry single_sample``, and a second pass using the full pipeline ``-entry single_sample_scenic``).

----

Single-sample Pipelines
***********************
Pipelines to run on a single sample or multiple samples separately and in parallel.

**single_sample** |single_sample|
----------------------------------

.. |single_sample| image:: https://github.com/vib-singlecell-nf/vsn-pipelines/workflows/single_sample/badge.svg

The ``single_sample`` workflow will process 10x data, taking in 10x-structured data, and metadata file.
The standard analysis steps are run: filtering, normalization, log-transformation, HVG selection, dimensionality reduction, clustering, and loom file generation.
The output is a loom file with the results embedded.

|Single-sample Workflow|

.. |Single-sample Workflow| image:: https://raw.githubusercontent.com/vib-singlecell-nf/vsn-pipelines/master/assets/images/single_sample.svg?sanitize=true


----

**single_sample_scenic** |single_sample_scenic|
-----------------------------------------------

.. |single_sample_scenic| image:: https://github.com/vib-singlecell-nf/vsn-pipelines/workflows/single_sample_scenic/badge.svg

Runs the ``single_sample`` workflow above, then runs the ``scenic`` workflow on the output, generating a comprehensive loom file with the combined results.
This could be very resource intensive, depending on the dataset.

|Single-sample SCENIC Workflow|

.. |Single-sample SCENIC Workflow| image:: https://raw.githubusercontent.com/vib-singlecell-nf/vsn-pipelines/master/assets/images/single_sample_scenic.svg?sanitize=true


----

**single_sample_scrublet** |single_sample_scrublet|
---------------------------------------------------

.. |single_sample_scrublet| image:: https://github.com/vib-singlecell-nf/vsn-pipelines/workflows/single_sample_scrublet/badge.svg

Runs the ``single_sample`` workflow above together with the Scrublet workflow.

|Single-sample Scrublet Workflow|

.. |Single-sample Scrublet Workflow| image:: https://raw.githubusercontent.com/vib-singlecell-nf/vsn-pipelines/master/assets/images/single_sample_scrublet.svg?sanitize=true


The ``single_sample`` workflow is running from the input data.
The ``scrublet`` workflow is running from the input data.
The final processed file from the ``single_sample`` pipeline is annotated with the cell-based data generated by Scrublet.

The pipelines generate the following relevant files for each sample:

.. list-table:: Output Files (not exhaustive list)
    :widths: 10 40
    :header-rows: 1

    * - Output File
      - Description
    * - `out/data/*.SINGLE_SAMPLE_SCRUBLET.loom`
      - `SCope`-ready loom file containing resulting loom file from a `single_sample` workflow but with additional metadata (doublet scores and predicted doublet for the cells) based on Scrublet run.
    * - `out/data/scrublet/*.SC__SCRUBLET__DOUBLET_DETECTION.ScrubletObject.pklz`
      - Pickled file containing the Scrublet object.
    * - `out/data/scrublet/*.SCRUBLET.SC__ANNOTATE_BY_CELL_METADATA.h5ad`
      - `h5ad` file with raw data and doublets annotated.
    * - `out/data/scrublet/*.SINGLE_SAMPLE_SCRUBLET.h5ad`
      - `h5ad` file resulting from a ``single_sample`` workflow run and with doublets (inferred from Scrublet) removed.

----

**decontx** |decontx|
---------------------

.. |decontx| image:: https://github.com/vib-singlecell-nf/vsn-pipelines/workflows/decontx/badge.svg

Runs the ``decontx`` workflow.

|DecontX Workflow|

.. |DecontX Workflow| image:: https://raw.githubusercontent.com/vib-singlecell-nf/vsn-pipelines/master/assets/images/decontx.svg?sanitize=true


The pipelines generate the following files for each sample:

.. list-table:: Output Files
    :widths: 10 40
    :header-rows: 1

    * - Output File
      - Description
    * - `out/data/*.CELDA_DECONTX_{FILTER,CORRECT}.h5ad`
      - A `h5ad` file with either the filtered matrix using one of the provided filters or the corrected (decontaminated) matrix by DecontX.
    * - `out/data/celda/*.CELDA__DECONTX.Rds`
      - A `Rds` file containing the SingleCellExperiment object processed by DecontX.
    * - `out/data/celda/*.CELDA__DECONTX.Contamination_Outlier_Table.tsv`
      - A cell-based `.tsv` file containing data generated by DecontX and additional outlier masks:

        - decontX_contamination
        - decontX_clusters
        - celda_decontx__{doublemad,scater_isOutlier_3MAD,custom_gt_0.5}_predicted_outliers
    * - `out/data/celda/*.CELDA__DECONTX.Contamination_Outlier_Thresholds.tsv`
      - A `.tsv` containing a table with the different threshold for generating the outlier masks.
    * - `out/data/celda/*.CELDA__DECONTX.Contamination_Score_Density_with_{doublemad,scater_isOutlier_3MAD,custom_gt_0.5}.pdf`
      - A `.pdf` plot showing the density of the decontamination score from DecontX and the outlier area highlighted for the given outlier threshold.
    * - `out/data/celda/*.CELDA__DECONTX.UMAP_Contamination_Score.pdf`
      - A `.pdf` plot showing the DecontX contamination score on top of a UMAP generated from the decontaminated matrix. 
    * - `out/data/celda/*.CELDA__DECONTX.UMAP_Clusters.pdf`
      - A `.pdf` plot showing a UMAP generated by DecontX and from the decontaminated matrix.

----

**single_sample_decontx** |single_sample_decontx|
-------------------------------------------------

.. |single_sample_decontx| image:: https://github.com/vib-singlecell-nf/vsn-pipelines/workflows/single_sample_decontx/badge.svg

Runs the ``single_sample`` workflow above together with the DecontX workflow.

|Single-sample DecontX Workflow|

.. |Single-sample DecontX Workflow| image:: https://raw.githubusercontent.com/vib-singlecell-nf/vsn-pipelines/master/assets/images/single_sample_decontx.svg?sanitize=true


The DecontX workflow is running from the input data.
The final processed file from the ``single_sample`` pipeline is annotated with the cell-based data generated by DecontX.

See ``single_sample`` and ``decontx`` to know more about the files generated by this pipeline.

----

**single_sample_decontx_scrublet** |single_sample_decontx_scrublet|
-------------------------------------------------------------------

.. |single_sample_decontx_scrublet| image:: https://github.com/vib-singlecell-nf/vsn-pipelines/workflows/single_sample_decontx_scrublet/badge.svg

Runs the ``single_sample`` workflow above together with the DecontX workflow.

|Single-sample DecontX Scrublet Workflow|

.. |Single-sample DecontX Scrublet Workflow| image:: https://raw.githubusercontent.com/vib-singlecell-nf/vsn-pipelines/master/assets/images/single_sample_decontx_scrublet.svg?sanitize=true


The ``single_sample`` workflow is running from the input data.
The ``decontx`` workflow is running from the input data.
The ``scrublet`` workflow is running from the output of the DecontX workflow.
The final processed file from the ``single_sample`` pipeline is annotated with the cell-based data generated by DecontX and Scrublet.

See ``single_sample``, ``decontx`` and ``scrublet`` to know more about the files generated by this pipeline.

----



**scenic** |scenic|
-------------------

.. |scenic| image:: https://github.com/vib-singlecell-nf/vsn-pipelines/workflows/scenic/badge.svg

Runs the ``scenic`` workflow alone, generating a loom file with only the SCENIC results.
Currently, the required input is a loom file (set by `params.sc.scenic.filteredLoom`).

|SCENIC Workflow|

.. |SCENIC Workflow| image:: https://raw.githubusercontent.com/vib-singlecell-nf/vsn-pipelines/master/assets/images/scenic.svg?sanitize=true


----

**scenic_multiruns** |scenic_multiruns| |single_sample_scenic_multiruns|
------------------------------------------------------------------------

.. |scenic_multiruns| image:: https://github.com/vib-singlecell-nf/vsn-pipelines/workflows/scenic_multiruns/badge.svg
.. |single_sample_scenic_multiruns| image:: https://github.com/vib-singlecell-nf/vsn-pipelines/workflows/single_sample_scenic_multiruns/badge.svg

Runs the ``scenic`` workflow multiple times (set by ``params.sc.scenic.numRuns``), generating a loom file with the aggregated results from the multiple SCENIC runs.

Note that this is not a complete entry-point itself, but a configuration option for the `scenic` module.
Simply adding `-profile scenic_multiruns` during the config step will activate this analysis option for any of the standard entrypoints.

|SCENIC Multi-runs Workflow|

.. |SCENIC Multi-runs Workflow| image:: https://raw.githubusercontent.com/vib-singlecell-nf/vsn-pipelines/master/assets/images/scenic_multiruns.svg?sanitize=true


----

**cellranger**
--------------
Runs the ``cellranger`` workflow (``makefastq``, then ``count``).
Input parameters are specified within the config file:

* ``params.sc.cellranger.mkfastq.csv``: path to the CSV samplesheet
* ``params.sc.cellranger.mkfastq.runFolder``: path of Illumina BCL run folder
* ``params.sc.cellranger.count.transcriptome``: path to the Cell Ranger compatible transcriptome reference

**cellranger_count_metadata**
-----------------------------

Given the data stored as:

.. code:: bash

    MKFASTQ_ID_SEQ_RUN1
    |-- MAKE_FASTQS_CS
     -- outs
        |-- fastq_path
            |-- HFLC5BBXX
                |-- test_sample1
                |   |-- sample1_S1_L001_I1_001.fastq.gz
                |   |-- sample1_S1_L001_R1_001.fastq.gz
                |   |-- sample1_S1_L001_R2_001.fastq.gz
                |   |-- sample1_S1_L002_I1_001.fastq.gz
                |   |-- sample1_S1_L002_R1_001.fastq.gz
                |   |-- sample1_S1_L002_R2_001.fastq.gz
                |   |-- sample1_S1_L003_I1_001.fastq.gz
                |   |-- sample1_S1_L003_R1_001.fastq.gz
                |   |-- sample1_S1_L003_R2_001.fastq.gz
                |-- test_sample2
                |   |-- sample2_S2_L001_I1_001.fastq.gz
                |   |-- sample2_S2_L001_R1_001.fastq.gz
                |   |-- ...
            |-- Reports
            |-- Stats
            |-- Undetermined_S0_L001_I1_001.fastq.gz
            ...
            -- Undetermined_S0_L003_R2_001.fastq.gz
    MKFASTQ_ID_SEQ_RUN2
    |-- MAKE_FASTQS_CS
     -- outs
        |-- fastq_path
            |-- HFLY8GGLL
                |-- test_sample1
                |   |-- ...
                |-- test_sample2
                |   |-- ...
            |-- ...


and a metadata table:

.. list-table:: Minimally Required Metadata Table
    :widths: 10 30 10 10 10
    :header-rows: 1

    * - sample_name
      - fastqs_parent_dir_path
      - fastqs_dir_name
      - fastqs_sample_prefix
      - expect_cells
    * - Sample1_Bio_Rep1
      - MKFASTQ_ID_SEQ_RUN1/outs/fastq_path/HFLY8GGLL
      - test_sample1
      - sample1
      - 5000
    * - Sample1_Bio_Rep1
      - MKFASTQ_ID_SEQ_RUN2/outs/fastq_path/HFLC5BBXX
      - test_sample1
      - sample1
      - 5000
    * - Sample1_Bio_Rep2
      - MKFASTQ_ID_SEQ_RUN1/outs/fastq_path/HFLY8GGLL
      - test_sample2
      - sample2
      - 5000
    * - Sample1_Bio_Rep2
      - MKFASTQ_ID_SEQ_RUN2/outs/fastq_path/HFLC5BBXX
      - test_sample2
      - sample2
      - 5000

Optional columns:

- ``short_uuid``: ``sample_name`` will be prefix by this value. This should be the same between sequencing runs of the same biological replicate
- ``expect_cells``: This number will be used as argument for the ``--expect-cells`` parameter in ``cellranger count``.
- ``chemistry``: This chemistry will be used as argument for the ``--chemistry`` parameter in ``cellranger count``.

and a config:

.. code:: bash

    nextflow config \
       ~/vib-singlecell-nf/vsn-pipelines \
       -profile cellranger_count_metadata \
       > nextflow.config

and a workflow run command:

.. code:: bash

    nextflow run \
        ~/vib-singlecell-nf/vsn-pipelines \
        -entry cellranger_count_metadata

The workflow will run Cell Ranger `count` on 2 samples, each using the 2 sequencing runs.

NOTES:

- If ``fastqs_dir_name`` does not exist, set it to ``none``

----

**demuxlet/freemuxlet**
-----------------------
Runs the ``demuxlet`` or ``freemuxlet`` workflows (``dsc-pileup`` [with prefiltering], then ``freemuxlet`` or ``demuxlet``)
Input parameters are specified within the config file:

* ``params.sc.popscle.vcf``: path to the VCF file for demultiplexing
* ``params.sc.popscle.freemuxlet.nSamples``: Number of clusters to extract (should match the number of samples pooled)
* ``params.sc.popscle.demuxlet.field``: Field in the VCF with genotype information


----

**nemesh**
----------
Runs the ``nemesh`` pipeline (Drop-seq) on a single sample or multiple samples separately.

`Source <http://mccarrolllab.org/wp-content/uploads/2016/03/Drop-seqAlignmentCookbookv1.2Jan2016.pdf>`_


----

Sample Aggregation Pipelines
****************************
Pipelines to aggregate multiple datasets together.

**bbknn** |bbknn|
-----------------

.. |bbknn| image:: https://github.com/vib-singlecell-nf/vsn-pipelines/workflows/bbknn/badge.svg

Runs the ``bbknn`` workflow (sample-specific filtering, merging of individual samples, normalization, log-transformation, HVG selection, PCA analysis, then the batch-effect correction steps: BBKNN, clustering, dimensionality reduction (UMAP only)).
The output is a loom file with the results embedded.

Source: https://github.com/Teichlab/bbknn/blob/master/examples/pancreas.ipynb

|BBKNN Workflow|

.. |BBKNN Workflow| image:: https://raw.githubusercontent.com/vib-singlecell-nf/vsn-pipelines/master/assets/images/bbknn.svg?sanitize=true


.. list-table:: Output Files (not exhaustive list)
    :widths: 10 40
    :header-rows: 1

    * - Output File
      - Description
    * - `out/data/*.BBKNN.loom`
      - `SCope`-ready loom file containing all results.
    * - `out/data/*.BBKNN.h5ad`
      - `Scanpy`-ready h5ad file containing all results.

----

**bbknn_scenic** |bbknn_scenic|
-------------------------------

.. |bbknn_scenic| image:: https://github.com/vib-singlecell-nf/vsn-pipelines/workflows/bbknn_scenic/badge.svg

Runs the ``bbknn`` workflow above, then runs the ``scenic`` workflow on the output, generating a comprehensive loom file with the combined results.
This could be very resource intensive, depending on the dataset.

|BBKNN SCENIC Workflow|

.. |BBKNN SCENIC Workflow| image:: https://raw.githubusercontent.com/vib-singlecell-nf/vsn-pipelines/master/assets/images/bbknn_scenic.svg?sanitize=true


.. list-table:: Output Files (not exhaustive list)
    :widths: 10 40
    :header-rows: 1

    * - Output File
      - Description
    * - `out/data/*.BBKNN.h5ad`
      - `Scanpy`-ready h5ad file containing all results from a `bbknn` workflow run.
    * - `out/data/*.BBKNN_SCENIC.loom`
      - `SCope`-ready loom file containing all results from a `bbknn` workflow and a `scenic` workflow run (e.g.: regulon AUC matrix, regulons, ...).

----

**harmony** |harmony|
----------------------

.. |harmony| image:: https://github.com/vib-singlecell-nf/vsn-pipelines/workflows/harmony/badge.svg

Runs the ``harmony`` workflow (sample-specific filtering, merging of individual samples, normalization, log-transformation, HVG selection, PCA analysis, batch-effect correction (Harmony), clustering, dimensionality reduction (t-SNE and UMAP)).
The output is a loom file with the results embedded.

|Harmony Workflow|

.. |Harmony Workflow| image:: https://raw.githubusercontent.com/vib-singlecell-nf/vsn-pipelines/master/assets/images/harmony.svg?sanitize=true

.. list-table:: Output Files (not exhaustive list)
    :widths: 10 40
    :header-rows: 1

    * - Output File
      - Description
    * - `out/data/*.HARMONY.loom`
      - `SCope`-ready loom file containing all results.
    * - `out/data/*.HARMONY.h5ad`
      - `Scanpy`-ready h5ad file containing all results.

----

**mnncorrect** |mnncorrect|
----------------------------

.. |mnncorrect| image:: https://github.com/vib-singlecell-nf/vsn-pipelines/workflows/mnncorrect/badge.svg

Runs the ``mnncorrect`` workflow (sample-specific filtering, merging of individual samples, normalization, log-transformation, HVG selection, PCA analysis, batch-effect correction (mnnCorrect), clustering, dimensionality reduction (t-SNE and UMAP)).
The output is a loom file with the results embedded.

----

|mnnCorrect Workflow|

.. |mnnCorrect Workflow| image:: https://raw.githubusercontent.com/vib-singlecell-nf/vsn-pipelines/master/assets/images/mnncorrect.svg?sanitize=true


.. list-table:: Output Files (not exhaustive list)
    :widths: 10 40
    :header-rows: 1

    * - Output File
      - Description
    * - `out/data/*.MNNCORRECT.loom`
      - `SCope`-ready loom file containing all results.
    * - `out/data/*.MNNCORRECT.h5ad`
      - `Scanpy`-ready h5ad file containing all results.


----

Utility Pipelines
*****************

Contrary to the aformentioned pipelines, these are not end-to-end. They are used to perfom small incremental processing steps.

**cell_annotate**
-----------------

Runs the ``cell_annotate`` workflow which will perform a cell-based annotation of the data using a set of provided .tsv metadata files.
We show a use case here below with 10x Genomics data were it will annotate different samples using the ``obo`` method. For more information
about this cell-based annotation feautre please visit `Cell-based metadata annotation`_ section.

.. _`Cell-based metadata annotation`: https://vsn-pipelines.readthedocs.io/en/latest/features.html#cell-based-metadata-annotation

First, generate the config :

.. code:: groovy

    nextflow config \
       ~/vib-singlecell-nf/vsn-pipelines \
       -profile tenx,utils_cell_annotate,singularity

Make sure the following parts of the generated config are properly set:

.. code:: bash

    [...]
    data {
      tenx {
         cellranger_mex = '~/out/counts/*/outs/'
      }
    }
    sc {
        scanpy {
            container = 'vibsinglecellnf/scanpy:0.5.2'
        }
        cell_annotate {
            off = 'h5ad'
            method = 'obo'
            indexColumnName = 'BARCODE'
            cellMetaDataFilePath = "~/out/data/*.best"
            sampleSuffixWithExtension = '_demuxlet.best'
            annotationColumnNames = ['DROPLET.TYPE', 'NUM.SNPS', 'NUM.READS', 'SNG.BEST.GUESS']
        }
        [...]
    }
    [...]

Now we can run it with the following command:

.. code:: groovy

    nextflow -C nextflow.config \
       run ~/vib-singlecell-nf/vsn-pipelines \
       -entry cell_annotate \
       > nextflow.config

**cell_annotate_filter**
------------------------

Runs the ``cell_annotate_filter`` workflow which will perform a cell-based annotation of the data using a set of provided .tsv metadata files following by a cell-based filtering.
We show a use case here below with 10x Genomics data were it will annotate different samples using the ``obo`` method. For more information
about this cell-based annotation feautre please visit `Cell-based metadata annotation`_ section and `Cell-based metadata filtering`_ section.

.. _`Cell-based metadata filtering`: https://vsn-pipelines.readthedocs.io/en/latest/features.html#cell-based-metadata-filtering

First, generate the config :

.. code:: groovy

    nextflow config \
       ~/vib-singlecell-nf/vsn-pipelines \
       -profile tenx,utils_cell_annotate,utils_cell_filter,singularity \
       > nextflow.config

Make sure the following parts of the generated config are properly set:

.. code:: bash

    [...]
    data {
      tenx {
         cellranger_mex = '~/out/counts/*/outs/'
      }
    }
    sc {
        scanpy {
            container = 'vibsinglecellnf/scanpy:0.5.2'
        }
        cell_annotate {
            off = 'h5ad'
            method = 'obo'
            indexColumnName = 'BARCODE'
            cellMetaDataFilePath = "~/out/data/*.best"
            sampleSuffixWithExtension = '_demuxlet.best'
            annotationColumnNames = ['DROPLET.TYPE', 'NUM.SNPS', 'NUM.READS', 'SNG.BEST.GUESS']
        }
        cell_filter {
            off = 'h5ad'
            method = 'internal'
            filters = [
                [
                    id:'NO_DOUBLETS',
                    sampleColumnName:'sample_id',
                    filterColumnName:'DROPLET.TYPE',
                    valuesToKeepFromFilterColumn: ['SNG']
                ]
            ]
        }
        [...]
    }
    [...]

Now we can run it with the following command:

.. code:: groovy

    nextflow -C nextflow.config \
       run ~/vib-singlecell-nf/vsn-pipelines \
       -entry cell_filter

Input Data Formats
*******************

Depending on the type of data you run the pipeline with, one or more appropriate profiles should be set when running ``nextflow config``.

All the input data parameters are compatible with the following features:

- Glob patterns

.. code::

    "data/10x/1k_pbmc/1k_pbmc_*/outs/"

- Comma separated paths (paths can contain glob patterns)

.. code::

    "data/10x/1k_pbmc/1k_pbmc_v2_chemistry/outs/, data/10x/1k_pbmc/1k_pbmc_v3_chemistry/outs/"

- Array of paths (paths can contain glob patterns)

.. code::

    [
        "data/10x/1k_pbmc/1k_pbmc_v2_chemistry/outs/",
        "data/10x/1k_pbmc/1k_pbmc_v3_chemistry/outs/"
    ]

----

Cell Ranger (10x Genomics)
--------------------------

Data from a standard Cell Ranger output directory can be easily ingested into the pipeline by using the proper input channel (``tenx_mex`` or ``tenx_h5``, depending on which file should be used).
Multiple samples can be selected by providing the path to this directory using glob patterns.

.. code::

    /home/data/
    └── cellranger
        ├── sample_A
        │   └── outs
        │       ├── filtered_feature_bc_matrix
        │       │   ├── barcodes.tsv
        │       │   ├── genes.tsv
        │       │   └── matrix.mtx
        │       └── filtered_feature_bc_matrix.h5
        └── sample_B
            └── outs
                ├── filtered_feature_bc_matrix
                │   ├── barcodes.tsv
                │   ├── genes.tsv
                │   └── matrix.mtx
                └── filtered_feature_bc_matrix.h5


MEX
___

To use the Cell Ranger Market Exchange (**MEX**) files, use the following profile when generating the config file::

    -profile tenx

This profile adds the following parameter (``params.data.tenx.cellranger_mex``) into the generated .config file::

    [...]
    data {
        tenx {
            cellranger_mex = "/home/data/cellranger/sample*/outs/"
        }
    }
    [...]


H5
__

To use the Cell Ranger ``h5`` file as input, use the following profile::

    -profile tenx_h5

This profile adds the ``params.data.tenx.cellranger_h5`` parameter into the generated .config file::

    [...]
    data {
        tenx {
            cellranger_h5 = "/home/data/cellranger/sample*/outs/"
        }
    }
    [...]


Input file detection
____________________

Setting the input directory appropriately, using a glob in the directory path in place of the sample names, will collect all the samples listed in the ``filtered_[feature|gene]_bc_matrix`` directories listed above.
For example, in ``params.data.tenx``, setting::

    cellranger_mex = "/home/data/cellranger/sample*/outs/"

or

.. code::

    cellranger_h5 = "/home/data/cellranger/sample*/outs/"

will recursively find all 10x samples in that directory.

The pipeline will use either the ``outs/filtered_feature_bc_matrix/`` or the ``outs/raw_feature_bc_matrix/`` depending on the setting of the ``params.sc.file_converter.useFilteredMatrix`` (``true`` uses filtered; ``false`` uses raw).

----

H5AD (Scanpy)
-------------
Use the following profile when generating the config file::

    -profile h5ad


In the generated .config file, make sure the ``file_paths`` parameter is set with the paths to the ``.h5ad`` files::

    [...]
    data {
        h5ad {
            file_paths = "data/1k_pbmc_v*_chemistry_SUFFIX.SC__FILE_CONVERTER.h5ad"
            suffix = "_SUFFIX.SC__FILE_CONVERTER.h5ad"
        }
    }
    [...]

- The ``suffix`` parameter is used to infer the sample name from the file paths (it is removed from the input file path to derive a sample name).

In case there are multiple .h5ad files that need to be processed with different suffixes, the multi-labelled strategy should be used to define the h5ad param::

    [...]
    data {
        h5ad {
            GROUP1 {
                file_paths = "[path-to-group1-files]/*.SUFFIX1.h5ad"
                suffix = ".SUFFIX1.h5ad"
            }
            GROUP2 {
                file_paths = "[path-to-group1-files]/*.SUFFIX2.h5ad"
                suffix = ".SUFFIX2.h5ad"
            }
        }
    }
    [...]

Notes: 

- ``GROUP1``, ``GROUP2`` are just example names here. They can be replaced by any value as long as they are alphanumeric (underscores are allowed).
- ``file_paths`` and ``suffix`` do allow list of paths/globs in the multi-labelled strategy.

----

Loom
----
Use the following profile when generating the config file::

    -profile loom


In the generated .config file, make sure the ``file_paths`` parameter is set with the paths to the ``.loom`` files::

    [...]
    data {
        loom {
            file_paths = "data/1k_pbmc_v*_chemistry_SUFFIX.SC__FILE_CONVERTER.loom"
            suffix = "_SUFFIX.SC__FILE_CONVERTER.loom"
        }
    }
    [...]

- The ``suffix`` parameter is used to infer the sample name from the file paths (it is removed from the input file path to derive a sample name).

----

Seurat Rds
----------

Use the following profile when generating the config file::

    -profile seurat_rds


In the generated .config file, make sure the ``file_paths`` parameter is set with the paths to the ``.Rds`` files::

    [...]
    data {
        seurat_rds {
            file_paths = "data/1k_pbmc_v*_chemistry_SUFFIX.SC__FILE_CONVERTER.Rds"
            suffix = "_SUFFIX.SC__FILE_CONVERTER.Rds"
        }
    }
    [...]

- The pipelines expect a Seurat v3 object contained in the .Rds file. (Seurat v2 objects are currently not supported).
- The ``suffix`` parameter is used to infer the sample name from the file paths (it is removed from the input file path to derive a sample name).

----

TSV
---
Use the following profile when generating the config file::

    -profile tsv


In the generated .config file, make sure the ``file_paths`` parameter is set with the paths to the ``.tsv`` files::

    [...]
    data {
        h5ad {
            file_paths = "data/1k_pbmc_v*_chemistry_SUFFIX.SC__FILE_CONVERTER.tsv"
            suffix = "_SUFFIX.SC__FILE_CONVERTER.tsv"
        }
    }
    [...]

- The ``suffix`` parameter is used to infer the sample name from the file paths (it is removed from the input file path to derive a sample name).

----

CSV
---
Use the following profile when generating the config file::

    -profile csv


In the generated .config file, make sure the ``file_paths`` parameter is set with the paths to the ``.csv`` files::

    [...]
    data {
        h5ad {
            file_paths = "data/1k_pbmc_v*_chemistry_SUFFIX.SC__FILE_CONVERTER.csv"
            suffix = "_SUFFIX.SC__FILE_CONVERTER.csv"
        }
    }
    [...]

- The ``suffix`` parameter is used to infer the sample name from the file paths (it is removed from the input file path to derive a sample name).

